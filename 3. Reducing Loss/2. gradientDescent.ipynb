{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "Suppose we had the time and the computing resources to calculate the loss for all possible values of w<sub>1</sub>. For the kind of regression problems we've been examining, the resulting plot of loss vs. w<sub>1</sub> will always be convex. In other words, the plot will always be bowl-shaped, kind of like this:  \n",
    "\n",
    "<img src=\"../Images/Reducing Loss - Convex Graph.png\" alt=\"Convex Graph\" width=\"500\"/>  \n",
    "\n",
    "Convex problems have only one minimum; that is, only one place where the slope is exactly 0. That minimum is where the loss function converges.\n",
    "\n",
    "Calculating the loss function for every conceivable value of w<sub>1</sub> over the entire data set would be an inefficient way of finding the convergence point. Let's examine a better mechanism—very popular in machine learning—called gradient descent.\n",
    "\n",
    "The first stage in gradient descent is to pick a starting value (a starting point) for w<sub>1</sub>. The starting point doesn't matter much; therefore, many algorithms simply set w<sub>1</sub> to 0 or pick a random value. The following figure shows that we've picked a starting point slightly greater than 0:  \n",
    "\n",
    "<img src=\"../Images/Reducing Loss - Convex Graph Starting Point.png\" alt=\"Convex Graph Starting Point\" width=\"500\"/>  \n",
    "\n",
    "The gradient descent algorithm then calculates the gradient of the loss curve at the starting point. Here in the above figure, the gradient of the loss is equal to the derivative (slope) of the curve, and tells you which way is \"warmer\" or \"colder.\" When there are multiple weights, the gradient is a vector of partial derivatives with respect to the weights.  \n",
    "\n",
    "The gradient always points in the direction of steepest increase in the loss function. The gradient descent algorithm takes a step in the direction of the negative gradient in order to reduce loss as quickly as possible.  \n",
    "\n",
    "<img src=\"../Images/Reducing Loss - Convex Graph With Negative Weight.png\" alt=\"Convex Graph Negative Weight\" width=\"500\"/>  \n",
    "\n",
    "To determine the next point along the loss function curve, the gradient descent algorithm adds some fraction of the gradient's magnitude to the starting point as shown in the following figure:  \n",
    "\n",
    "<img src=\"../Images/Reducing Loss - Convex Graph Next Point.png\" alt=\"Convex Graph Next Point\" width=\"500\"/>  \n",
    "\n",
    "The gradient descent then repeats this process, edging ever closer to the minimum."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fabf7593ccb2784a7f08653d7fe2c34623df00277caf694a484c7ed036489b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
