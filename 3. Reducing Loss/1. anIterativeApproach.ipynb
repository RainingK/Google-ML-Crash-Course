{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Iterative Approach\n",
    "You've learnt what loss is and now you'll learn how to reduce loss.  \n",
    "\n",
    "Iterative learning is like the game \"Hot and Cold\". In this game, the \"hidden object\" is the best possible model. You'll start with a wild guess (\"The value of w<sub>1</sub> is 0\") and wait for the system to tell you what the loss is. Then, you'll try another guess (\"The value of w<sub>1</sub> is 0.5.\") and see what the loss is. Aah, you're getting warmer. Actually, if you play this game right, you'll usually be getting warmer. The real trick to the game is trying to find the best possible model as efficiently as possible.  \n",
    "\n",
    "![Reducing Loss - Iterative Approach](../Images/Reducing%20Loss%20-%20Iterative%20Approach.png)  \n",
    "\n",
    "## An Iterative Approach To Training A Model\n",
    "The \"model\" takes one or more features as input and returns one prediction () as output. To simplify, consider a model that takes one feature and returns one prediction:  \n",
    "y = b + w<sub>1</sub>x<sub>1</sub>  \n",
    "\n",
    "The initial values would be 0 so  \n",
    "b = 0  \n",
    "w<sub>1</sub> = 0  \n",
    "The feature value would be 10 so our predicted label would be  \n",
    "y = 0 + 0 * 10  \n",
    "y = 0  \n",
    "\n",
    "The \"Compute Loss\" part of the diagram is the loss function that the model will use. Suppose we use the squared loss function. The loss function takes in two input values:\n",
    "* The model's prediction for features x.\n",
    "* The correct label corresponding to features x.  \n",
    "\n",
    "At last, we've reached the \"Compute parameter updates\" part of the diagram. It is here that the machine learning system examines the value of the loss function and generates new values for  and . For now, just assume that this mysterious box devises new values and then the machine learning system re-evaluates all those features against all those labels, yielding a new value for the loss function, which yields new parameter values. And the learning continues iterating until the algorithm discovers the model parameters with the lowest possible loss. Usually, you iterate until overall loss stops changing or at least changes extremely slowly. When that happens, we say that the model has **converged**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fabf7593ccb2784a7f08653d7fe2c34623df00277caf694a484c7ed036489b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
